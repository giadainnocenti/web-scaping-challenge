{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser, exceptions\n",
    "import pandas as pd\n",
    "#import os\n",
    "#import time\n",
    "import requests\n",
    "import re\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open chrome driver browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_check_url(url, browser_to_use = 'chrome', text_check = 'Mars'):\n",
    "    #set up the browser to use\n",
    "    browser = Browser(browser_to_use)\n",
    "    #visit the given url\n",
    "    browser.visit(url)\n",
    "    #it returns a positive message if there is text in the url and a negative one if it is empty\n",
    "    if browser.is_text_present(text_check):\n",
    "        print(\"Yes, the official website was found!\")\n",
    "    else:\n",
    "        print(\"No, it wasn't found...\")\n",
    "    return(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the official website was found!\n",
      "Mars Now\n",
      "Now uncocooned from its protective carbon-fiber shield, the helicopter is being readied for its next steps.  \n"
     ]
    }
   ],
   "source": [
    "url_nasa = 'https://mars.nasa.gov/news/'\n",
    "#check if you can find the website\n",
    "browser = visit_check_url(url_nasa)\n",
    "#creating a BEautifulSoup object\n",
    "soup = bs(browser.html, 'html.parser')\n",
    "#reading the latest News Title\n",
    "news_title = soup.find_all('div', class_='content_title')[0].text\n",
    "#reading the latest news body\n",
    "paragraph = soup.find_all('div', class_=\"article_teaser_body\")[0].text\n",
    "#printing the title and body just to see them and check that they are correct\n",
    "print(f'{news_title}\\n{paragraph}')\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the official website was found!\n",
      "https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/image/featured/mars1.jpg\n"
     ]
    }
   ],
   "source": [
    "url_jpl = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "#check if you can find the website\n",
    "browser = visit_check_url(url_jpl)\n",
    "#clicking on the button FULL IMAGE to see the full image featured this month  class=\"btn btn-outline-light\"\n",
    "browser.links.find_by_partial_text('FULL IMAGE').click()\n",
    "#scraping the page with BS\n",
    "soup = bs(browser.html, 'html.parser')\n",
    "#saving the image url, sometimes it does not find mars3.jpg I am not sure why\n",
    "featured_image_url = url_jpl.\\\n",
    "    replace(\"index.html\", soup.find(class_=\"fancybox-image\")[\"src\"]) \n",
    "print(featured_image_url)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the official website was found!\n"
     ]
    }
   ],
   "source": [
    "url_mars_facts = 'https://space-facts.com/mars/'\n",
    "#check if you can find the website\n",
    "browser = visit_check_url(url_mars_facts)\n",
    "browser.quit()\n",
    "# reading the table from the url using pandas\n",
    "table = pd.read_html(url_mars_facts)\n",
    "#selecting the table containing Mars information\n",
    "df_facts = table[0]\n",
    "#renaming the columns\n",
    "df_facts.columns = ['Description', 'Value']\n",
    "#saving the table as an HTML and stripping \\n character not known by html.\n",
    "html_facts = df_facts.to_html(classes=\"table table-dark\")\n",
    "html_facts = html_facts.replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the official website was found!\n"
     ]
    }
   ],
   "source": [
    "base_url_hemisphere = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser = visit_check_url(url_hemisphere)\n",
    "html = browser.html\n",
    "#check if you can find the website\n",
    "soup = bs(html, 'html.parser')\n",
    "#finding all the items and possible images\n",
    "items = soup.find_all(class_='item')\n",
    "#getting information on the images such as title and urls to find the full image\n",
    "#titles = []\n",
    "#images_urls = []\n",
    "hemispheres = []\n",
    "for item in items:\n",
    "    item_dic = {}\n",
    "    #finding the titles of the images\n",
    "    title = item.find('h3').text\n",
    "    item_dic['Title'] = title\n",
    "    #clicking in every image to retrieve the full size picture\n",
    "    browser.links.find_by_partial_text(title).click()\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "    #finding the urls of the picture\n",
    "    url_1 = soup.find('div', class_='downloads')\n",
    "    url = url_1.find('a')['href']\n",
    "    item_dic['img_url'] = url\n",
    "    browser.back()\n",
    "    hemispheres.append(item_dic)\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
